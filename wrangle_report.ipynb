{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: Wragle_report\n",
    "\n",
    "Over the course of this analysis, we are gathering data from a variety of sources, including scrapping data from @WeRateDogs Twitter API, and assessing its quality and tidiness both visually and programmatically, then cleaning it.\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#gathering\">Data Gathering</a></li>\n",
    "<li><a href=\"#assessing\">Assessing Data</a></li>\n",
    "<li><a href=\"#cleaning\">Cleaning Data</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gathered data from three different data sources and combined them together into one final master dataset in order to conduct further analyses and visualizations. Specifically, data comes from twitter_archive_enhanced.csv file, image_predictions.tsv file and @WeRateDogs Twitter API.  \n",
    "\n",
    "Firstly, the WeRateDogs Twitter archive data contains basic tweet data (tweet ID, timestamp, dog name, dog stage, text, etc.) with more than 2k tweets and their ratings over the period from 2015 to August 1, 2017. This file was downloaded manually by clicking the provided link by Udacity in the form of csv file. Once the file was downloaded, I uploaded it to my Jupyter Notebook Workspace and readed the file into a pandas DataFrame.\n",
    "\n",
    "Secondly, I used the Requests library to download programmatically the tweet image predictions 'image_predictions.tsv' file via this [url](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') link hosted on Udacity's servers. This dataset contains three image predictions alongside with their tweet ID, image url, three highest prediction confidences and the image number that corresponded to the highest confident prediction.\n",
    "\n",
    "Lastly, I used the Tweepy library to query additional data via the Twitter API (tweet_json.txt). This Twitter API contains information about the retweet_count and favorite_count for each tweet in @WeRateDogs. To scrap data off from Twitter API, I had to apply for a developer account to acquire the Twitter API keys, secrets, and tokens for the authentication step. Then I was able to use the Tweepy library to query @WeRateDogs tweets. After scrapping data from Twitter API, we store each tweet into a json file called tweet_json.txt by reading tweet_json.txt line by line into lists with keys including tweet_id, retweet_count and favorite_count.\n",
    "\n",
    "These three datasets are later combined together into 'twitter_archive_master.csv' file and converted to the final data frame in Pandas based on the common tweet ID key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Assessing Data\n",
    "\n",
    "During this assessment step, I detected, assessed and documented quality and tidiness issues by using both visual assessment\n",
    "programmatic assessement. Below is the list of data quality issues and tidiness issues that I encountered:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "#### Dataframe for  'twitter_archive_enhanced.csv' file (alias df1)\n",
    "* Tweet ids are integers while they should be in form of strings.\n",
    "\n",
    "* Replies and retweets' records are incomplete, 78 and 181 non-null respectively.\n",
    "\n",
    "* Timestamp has wrong data type, string instead of datetime.\n",
    "\n",
    "* Inaccurate value exists in rating_denomenator field, which should equal 10.\n",
    "\n",
    "* Rating_numerator and rating_denomenator are two columns to measure rating of a tweet.\n",
    "\n",
    "* Some dog's unidentifed names such as a,an,the will be replaced to be null values instead.\n",
    "\n",
    "* Source column contain 2352 duplicated values. This column is therefore not helpful for analysis and need to be dropped out.\n",
    "\n",
    "#### Dataframe for  @WeRateDogs Twitter data (alias df2)\n",
    "* Tweet ids are integers while they should be strings.\n",
    "\n",
    "#### Dataframe for 'image_predictions.tsv' file \n",
    "* Tweet ids have wrong data types. \n",
    "\n",
    "* There are predictions that turn out to be non-dogs.\n",
    "\n",
    "### Tidiness issues\n",
    "\n",
    "* Four columns in df1 represent one unit information about the phase of a dog. \n",
    "\n",
    "* Merging all three datasets together using tweet_id as the common key to make the dataset for analysis complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Cleaning Data\n",
    "\n",
    "In this final step of data wrangling, I cleaned all of the issues I documented in the assessment session.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the cleaning process, first of all, I made copies of original datasets to make sure I do not make changes to the original data. Next, I applied the Define-Code-Test framework to conduct my cleansing. It means that I started by defining the methods/steps I used to fix the issues, then wrote code to clean them and finally tested if my cleaning was successful. \n",
    "* After cleaning process, I merged the three datasets into one final master dataset and exported it to 'twitter_archive_master.csv' file and then loaded it into a Pandas dataframe for analyzing and visulizing part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
